{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitish6121999/US-HOME-PRICE-PREDICTION/blob/main/Home_LLC__Modelling_Assignmnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - U.S Home Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Supervised Machine Learning Regression\n",
        "##### **Created by**      - Nitish N Naik"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/nitish6121999/US-HOME-PRICE-PREDICTION"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The aim of this study is to develop a robust predictive model for US home prices leveraging comprehensive datasets encompassing various economic, societal, and market-related factors. The objective is to accurately predict future home prices based on historical trends and the interplay of influential variables.\n",
        "\n",
        "###Steps Involved:\n",
        "\n",
        "**Data Collection**: Gather data from credible online sources encompassing US home pricing, real estate metrics, and a comprehensive set of factors known to influence housing markets.\n",
        "\n",
        "**Dataset Merging and Understanding**: Consolidate and merge datasets to create a unified dataset. Understand the structure, contents, and relationships within the data.\n",
        "\n",
        "**Focus on US Home Pricing and Real Estate**: Delve deep into the dynamics of US home pricing and real estate markets to comprehend patterns, trends, and influencing factors.\n",
        "\n",
        "**Factor Identification and Understanding**: Identify, analyze, and understand the multitude of factors affecting home prices as potential independent variables.\n",
        "\n",
        "**Exploratory Data Analysis (EDA)**: Perform exploratory data analysis to uncover correlations, trends, outliers, and patterns within the dataset. This step involves data cleaning, visualization, and statistical analysis.\n",
        "\n",
        "**Model Development**: Utilize machine learning or statistical modeling techniques to build a predictive model. Train the model using historical data and the identified factors to predict future home prices accurately.\n",
        "\n",
        "**Model Evaluation and Validation**: Validate the model's accuracy, robustness, and predictive capabilities using suitable metrics and validation techniques.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factors considered\n",
        "\n",
        "House price Index 2001-2023\n",
        "\n",
        "Average Sales Price of Houses Sold for the United States\n",
        "\n",
        "Consumer Price Index for All Urban Consumers Housing in U.S. City\n",
        "\n",
        "Average Economic Policy Uncertainty Index for United States (USEPUINDXD)\n",
        "\n",
        "Government subsidies Federal Housing L312051A027NBEA\n",
        "\n",
        "Homeownership Rate in the United States RHORUSQ156N\n",
        "\n",
        "Interest Rates and Price Indexes 2001-2023.csv\n",
        "\n",
        "Monthly Supply of New Houses in the United States\n",
        "\n",
        "Mortgage Average in the United States MORTGAGE30US\n",
        "\n",
        "Net housing value added Subsidies\n",
        "\n",
        "Public Transportation in U.S. City Average\n",
        "\n",
        "Real Median Household Income in the United States 2001-2022\n",
        "\n",
        "Unemployeement rate 2002-2023\n",
        "\n",
        "Working Age Population Aged 15-64"
      ],
      "metadata": {
        "id": "LRb5FQ1_yQ2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyJhz64l6lkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BorIKLLI6ngl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hpi=path= '/content/drive/MyDrive/Access file/Extras/HOME LLC/merged_dataset.csv'\n",
        "\n",
        "df_hpi = pd.read_csv(path, index_col=0)"
      ],
      "metadata": {
        "id": "R2On8YVqfokp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hpi.head(10)\n",
        "#df_hpi.drop(columns='unnamed',inplace=True)"
      ],
      "metadata": {
        "id": "KHQKKQcGgQMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df_hpi, diag_kind= 'kde')"
      ],
      "metadata": {
        "id": "sTa9ZEb6ZhX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "WE can understand how data is distributed across the dataset"
      ],
      "metadata": {
        "id": "pH5XQ1IGatjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_col= df_hpi.describe().columns"
      ],
      "metadata": {
        "id": "ad5Sgl8Jb1Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numeric_col[1:]:\n",
        "  fig = plt.figure(figsize=(9,6))\n",
        "  ax = fig.gca()\n",
        "  feature = df_hpi[col]\n",
        "  correlation = feature.corr(df_hpi['Price-index'])\n",
        "  plt.scatter(x=feature, y= df_hpi['Price-index'])\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Price-index')\n",
        "  ax.set_title('Price-index vs '+col+ ' with the correlation value : '+str(correlation))\n",
        "  z= np.polyfit(df_hpi[col],df_hpi['Price-index'],1)\n",
        "  y_hat = np.poly1d(z)(df_hpi[col])\n",
        "  plt.plot(df_hpi[col],y_hat,'r--',lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MVePe4wbZhUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "To understand the correlation of the columns wrt the dependent variable\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "From above scatter plot or Regression plot shows some numeric features has positive correlation with dependent variable and some has negative correlation\n",
        "\n",
        "correlation of features with dependent feature :--\n",
        "\n",
        "negative correlation = Homeownership rate, unemployment rate,fedral rate, monthly new house columns"
      ],
      "metadata": {
        "id": "Jv5H4eltd16u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,8))\n",
        "sns.heatmap(abs(df_hpi.corr()), cmap='coolwarm', annot=True)\n"
      ],
      "metadata": {
        "id": "fOW-A6N6ZhRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multicolinearity variables (using VIF method to deal with them)"
      ],
      "metadata": {
        "id": "eTxIgDDpzuMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "   vif = pd.DataFrame()\n",
        "   vif[\"variables\"] = X.columns\n",
        "   vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "   return(vif)\n",
        "\n",
        "\n",
        "calc_vif(df_hpi[[i for i in df_hpi.describe().columns if i not in ['Price-index','Population','Year','govt_subsidies']]])"
      ],
      "metadata": {
        "id": "28dlV40BZhO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hpi.drop(columns='population',inplace=True)\n",
        "df_hpi.drop(columns='Year',inplace=True)\n",
        "df_hpi.drop(columns='govt_subsidies',inplace=True)"
      ],
      "metadata": {
        "id": "Xg5cvD0vZhME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hpi.shape"
      ],
      "metadata": {
        "id": "h5EFc82-ZhJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,8))\n",
        "sns.heatmap(abs(df_hpi.corr()), cmap='coolwarm', annot=True)"
      ],
      "metadata": {
        "id": "AXZlAXrMZhGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Building"
      ],
      "metadata": {
        "id": "h5hpq_5pz8K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ridge_regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "ZfUtfY1WhYLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= df_hpi.drop(columns=['Price-index'])\n",
        "\n",
        "y=np.sqrt(df_hpi['Price-index'])\n",
        "\n",
        "\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.20,random_state=10)\n",
        "\n",
        "\n",
        "xtrain.shape,xtest.shape,ytrain.shape,ytest.shape"
      ],
      "metadata": {
        "id": "wilWSplphZ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to different algorithms"
      ],
      "metadata": {
        "id": "cclwXKtK0BFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Appending all models parameters to the corrosponding list\n",
        "mean_absolut_error = []\n",
        "mean_sq_error=[]\n",
        "root_mean_sq_error=[]\n",
        "training_score =[]\n",
        "r2_list=[]\n",
        "adj_r2_list=[]\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "def score_metrix (model,X_train,X_test,Y_train,Y_test):\n",
        "\n",
        "  '''\n",
        "    train the model and gives mae, mse,rmse,r2,adj r2 score of the model\n",
        "\n",
        "  '''\n",
        "  #training the model\n",
        "  model.fit(X_train,Y_train)\n",
        "\n",
        "  # Training Score\n",
        "  training  = model.score(X_train,Y_train)\n",
        "  print(\"Training score  =\", training)\n",
        "\n",
        "  try:\n",
        "      # finding the best parameters of the model if any\n",
        "    print(f\"The best parameters found out to be :{model.best_params_} \\nwhere model best score is:  {model.best_score_} \\n\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "  #predicting the Test set and evaluting the models\n",
        "\n",
        "  if model == LinearRegression() or model == Lasso() or model == Ridge():\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    #finding mean_absolute_error\n",
        "    MAE  = mean_absolute_error(Y_test**2,Y_pred**2)\n",
        "    print(\"MAE :\" , MAE)\n",
        "\n",
        "    #finding mean_squared_error\n",
        "    MSE  = mean_squared_error(Y_test**2,Y_pred**2)\n",
        "    print(\"MSE :\" , MSE)\n",
        "\n",
        "    #finding root mean squared error\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "    #finding the r2 score\n",
        "\n",
        "    r2 = r2_score(Y_test**2,Y_pred**2)\n",
        "    print(\"R2 :\" ,r2)\n",
        "    #finding the adjusted r2 score\n",
        "    adj_r2=1-(1-r2_score(Y_test**2,Y_pred**2))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "    print(\"Adjusted R2 : \",adj_r2,'\\n')\n",
        "\n",
        "  else:\n",
        "    # for tree base models\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    #finding mean_absolute_error\n",
        "    MAE  = mean_absolute_error(Y_test,Y_pred)\n",
        "    print(\"MAE :\" , MAE)\n",
        "\n",
        "    #finding mean_squared_error\n",
        "    MSE  = mean_squared_error(Y_test,Y_pred)\n",
        "    print(\"MSE :\" , MSE)\n",
        "\n",
        "    #finding root mean squared error\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "    #finding the r2 score\n",
        "\n",
        "    r2 = r2_score(Y_test,Y_pred)\n",
        "    print(\"R2 :\" ,r2)\n",
        "    #finding the adjusted r2 score\n",
        "    adj_r2=1-(1-r2_score(Y_test,Y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "    print(\"Adjusted R2 : \",adj_r2,'\\n')\n",
        "\n",
        "    try:\n",
        "\n",
        "      # ploting the graph of feature importance\n",
        "\n",
        "       best = model.best_estimator_\n",
        "       features = X_train.columns\n",
        "       importances = best.feature_importances_\n",
        "       indices = np.argsort(importances)[-5:]  # Selecting the top five most important features\n",
        "\n",
        "       plt.figure(figsize=(10, 8))  # Adjust figure size as needed\n",
        "       plt.title('Top 5 Feature Importance')\n",
        "       plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n",
        "       plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "       plt.xlabel('Relative Importance')\n",
        "       plt.show()\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  # Here we appending the parameters for all models\n",
        "  mean_absolut_error.append(MAE)\n",
        "  mean_sq_error.append(MSE)\n",
        "  root_mean_sq_error.append(RMSE)\n",
        "  training_score.append(training)\n",
        "  r2_list.append(r2)\n",
        "  adj_r2_list.append(adj_r2)\n",
        "\n",
        "  print('*'*80)\n",
        "  # print the cofficient and intercept of which model have these parameters and else we just pass them\n",
        "  try :\n",
        "    print(\"coefficient \\n\",model.coef_)\n",
        "    print('\\n')\n",
        "    print(\"Intercept  = \" ,model.intercept_)\n",
        "  except:\n",
        "    pass\n",
        "  print('\\n')\n",
        "  print('*'*20, 'ploting the graph of Actual and predicted only with 80 observation', '*'*20)\n",
        "\n",
        "  # ploting the graph of Actual and predicted only with 80 observation for better visualisation which model have these parameters and else we just pass them\n",
        "  try:\n",
        "    # ploting the line graph of actual and predicted values\n",
        "    plt.figure(figsize=(15,7))\n",
        "    plt.plot((Y_pred)[:80])\n",
        "    plt.plot((np.array(Y_test)[:80]))\n",
        "    plt.legend([\"Predicted\",\"Actual\"])\n",
        "    plt.show()\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "r6E2_jXLhbrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which Evaluation metrics did you consider for a positive business impact and why?\n",
        "\n",
        "### **Interpretability**:\n",
        "\n",
        "The R2 score provides a straightforward interpretation of the proportion of variance in the target variable explained by the model. It represents the model's ability to capture and explain the variation in the data. A higher R2 score indicates that the model can better predict the target variable, which is crucial for making informed business decisions.\n",
        "\n",
        "###Stakeholder Communication:\n",
        "\n",
        "The R2 score is a widely understood metric and can be easily communicated to stakeholders, including non-technical individuals. It allows you to explain the model's performance in a concise and intuitive manner, enabling effective communication of the value and effectiveness of the predictive model.\n",
        "\n",
        "###Decision-making Support:\n",
        "\n",
        "A high R2 score suggests that the model is capturing a significant amount of the underlying patterns and relationships in the data. This can provide valuable insights for decision-making processes within a business context.\n",
        "\n",
        "###Model Comparison:\n",
        "\n",
        "The R2 score facilitates the comparison of different regression models or variations of the same model. It allows you to assess the relative performance of different approaches or configurations and choose the one that provides the best predictive power."
      ],
      "metadata": {
        "id": "HQQWio055rJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression"
      ],
      "metadata": {
        "id": "zpFSdwxn0KVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix (LinearRegression(),xtrain,xtest,ytrain,ytest)"
      ],
      "metadata": {
        "id": "i7anPpU2hbo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regression"
      ],
      "metadata": {
        "id": "KVtbZAbVp3J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer()\n",
        "xtrain_trans = pt.fit_transform(xtrain)      # fit transform the training set\n",
        "xtest_trans = pt.transform(xtest)\n",
        "\n",
        "L2 = Ridge()\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100,0.5,1.5,1.6,1.7,1.8,1.9]}      # giving parameters\n",
        "L2_cv = GridSearchCV(L2, parameters, scoring='r2', cv=5)                                                                    #using gridsearchcv and cross validate the model\n",
        "score_metrix(L2_cv,xtrain_trans,xtest_trans,ytrain,ytest)"
      ],
      "metadata": {
        "id": "goLbBW_Fhbme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso Regression"
      ],
      "metadata": {
        "id": "rr4rYDtLp7wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L1 = Lasso() #creating variable\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014]} #lasso parameters\n",
        "lasso_cv = GridSearchCV(L1, parameters, cv=5) #using gridsearchcv and cross validate the model\n",
        "\n",
        "\n",
        "score_metrix(lasso_cv,xtrain_trans,xtest_trans,ytrain,ytest)\n"
      ],
      "metadata": {
        "id": "hAWoIZk8hbj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4XrBb7ELhbhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random forest regression model"
      ],
      "metadata": {
        "id": "4RQf5e8UqqEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_x=df_hpi.drop(columns='Price-index')\n",
        "new_y=df_hpi['Price-index']\n",
        "\n",
        "\n",
        "new_xtrain, new_xtest, new_ytrain, new_ytest = train_test_split(new_x, new_y, test_size= 0.20, random_state = 10)\n",
        "\n",
        "\n",
        "new_xtrain.shape,   new_xtest.shape,   new_ytrain.shape,  new_ytest.shape\n"
      ],
      "metadata": {
        "id": "kNVMzyBdhbed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# parameters for random forest regression model\n",
        "\n",
        "rf_param_grid ={\"n_estimators\":[50,100,150],                    ### we can put any values for parameters\n",
        "              'max_depth' : [10,15,20,25,'none'],\n",
        "              'min_samples_split': [10,50,100],\n",
        "              'max_features' :[24,35,40,49]}\n",
        "\n",
        "\n",
        "# Using grid search cv\n",
        "Ranom_forest_Grid_search = GridSearchCV(RandomForestRegressor(),param_grid=rf_param_grid,n_jobs=-1,verbose=2)"
      ],
      "metadata": {
        "id": "15yXwnuFhbb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix(Ranom_forest_Grid_search, new_xtrain, new_xtest, new_ytrain, new_ytest)"
      ],
      "metadata": {
        "id": "1GcHVqxehbZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decission Tree Regressor model"
      ],
      "metadata": {
        "id": "tTG560SPrEPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "\n",
        "# Parameters for Decission Tree model\n",
        "param_grid = {'criterion' : [\"squared_error\"],\n",
        "              'splitter' : [\"best\", \"random\"],\n",
        "              'max_depth' : [10,15,25, 'none'],\n",
        "              'min_samples_split': [10,50,100],\n",
        "              'max_features' :[24,35,40,49]}\n",
        "\n",
        "# Gridsearch CV\n",
        "Dt_grid_search = GridSearchCV(DecisionTreeRegressor(),param_grid=param_grid,cv=2,n_jobs=-1)\n",
        "\n",
        "\n",
        "score_metrix(Dt_grid_search,new_xtrain,new_xtest,new_ytrain,new_ytest)"
      ],
      "metadata": {
        "id": "14Quinz8hbWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conclusion\n",
        "\n",
        "**If simplicity and interpretability** are important, linear models like Linear Regression, Ridge, or Lasso might be favorable.\n",
        "\n",
        "If **predictive accuracy** is the priority and interpretability is less critical, Random Forest or Decision Tree might be considered.\n",
        "\n",
        "**Regularization (Ridge/Lasso)** helps when you suspect overfitting.\n",
        "\n",
        "The balance between bias and variance needs consideration. Linear models have lower variance but might have higher bias compared to ensemble methods like Random Forest.\n",
        "\n",
        "Considering the overall balance between the given metrics, Ridge or Lasso Regression might be suitable as they offer a balance between good performance and regularization to mitigate overfitting."
      ],
      "metadata": {
        "id": "NMD5ZfC_tjLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MODEL EXPLAINABILITY"
      ],
      "metadata": {
        "id": "DBfKqONCvdqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1=LinearRegression()\n"
      ],
      "metadata": {
        "id": "g6zzu_XY2oQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(xtrain,ytrain)"
      ],
      "metadata": {
        "id": "1DQunwKghbO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "YfN_2JQrhbMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "explainer = shap.KernelExplainer(model1.predict, xtrain)"
      ],
      "metadata": {
        "id": "b2dhlQnuvBBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values = explainer.shap_values(xtest)"
      ],
      "metadata": {
        "id": "_S-hHjDbwU9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values, xtest,feature_names=numeric_col[1:])\n"
      ],
      "metadata": {
        "id": "Gx0rXSYBwU5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "-Ma_x3icwsu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model into pickle file for deployement process\n",
        "pickle.dump(model1, open('model1.pkl', 'wb'))\n"
      ],
      "metadata": {
        "id": "9_c8A2LewU07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the exploration of U.S. home price prediction spanning two decades, multiple regression models were deployed, including linear regression, Lasso and Ridge regularization, Random Forest Regressor, and Decision Tree Regressor. These models underwent a thorough evaluation based on diverse metrics.\n",
        "\n",
        "Among the various models examined, Lasso regularization emerged as a standout performer in predicting U.S. home prices. Its consistent and robust performance underscores its ability to capture intricate patterns within the dataset effectively.\n",
        "\n",
        "Additionally, the application of the SHAP (SHapley Additive exPlanations) tool for model interpretability illuminated the influential factors impacting U.S. home prices. This tool provided invaluable insights into understanding the factors that significantly influence the housing market.\n",
        "\n",
        "An interesting revelation from this comprehensive analysis was the identification of certain features that hold minimal or negligible impact on U.S. home prices. This nuanced understanding of influential factors, coupled with the selection of the most accurate model, holds immense potential to guide informed decision-making processes within the dynamic landscape of real estate."
      ],
      "metadata": {
        "id": "sBS7-cmG1i3s"
      }
    }
  ]
}